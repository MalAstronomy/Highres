{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Read line opacities of H2O_main_iso...\n",
      " Done.\n",
      "  Read line opacities of CO_main_iso...\n",
      " Done.\n",
      "  Read line opacities of CO_36...\n",
      " Done.\n",
      "\n",
      "  Read CIA opacities for H2-H2...\n",
      "  Read CIA opacities for H2-He...\n",
      "Done.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as sched\n",
    "import wandb\n",
    "\n",
    "from dawgz import job, after, ensure, schedule\n",
    "from itertools import chain, islice\n",
    "from pathlib import Path\n",
    "from torch import Tensor\n",
    "Tensor\n",
    "from tqdm import tqdm\n",
    "from typing import *\n",
    "import pandas as pd\n",
    "\n",
    "from lampe.data import H5Dataset\n",
    "from zuko.distributions import BoxUniform\n",
    "from lampe.inference import NPE, NPELoss\n",
    "from lampe.nn import ResMLP\n",
    "from zuko.flows import NAF, NSF, MAF, NCSF, SOSPF, UNAF, CNF \n",
    "from lampe.plots import nice_rc, corner, coverage_plot, mark_point\n",
    "from lampe.utils import GDStep\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, '/home/mvasist/Highres/simulations/')\n",
    "from DataProcuring import Data \n",
    "from ProcessingSpec import ProcessSpec\n",
    "from parameter import *\n",
    "from spectra_simulator import SpectrumMaker\n",
    "from parameter_set_script import param_set, param_list, param_list_ext, param_set_ext, deNormVal\n",
    "# from spectra_simulator import Simulator, LOWER, UPPER\n",
    "# from AverageEstimator import avgestimator\n",
    "\n",
    "# sys.path.insert(0, '/home/mvasist/Highres/sbi/added_scripts/')\n",
    "from added_scripts.corner_modified import *\n",
    "from added_scripts.pt_plotting import *\n",
    "\n",
    "\n",
    "# from ees import Simulator, LOWER, UPPER, LABELS, pt_profile\n",
    "LABELS, LOWER, UPPER = zip(*[\n",
    "[                  r'$FeH$',  -1.5, 1.5],   # temp_node_9\n",
    "[                  r'$CO$',  0.1, 1.6],  # CO_mol_scale\n",
    "[                  r'$\\log g$',   2.5, 5.5],          # log g\n",
    "[                  r'$Tint$',  300,   3500],   # temp_node_5\n",
    "[                  r'$T1$',  300,   3500],      # T_bottom\n",
    "[                  r'$T2$',  300,   3500],   # temp_node_1\n",
    "[                  r'$T3$',  300,   3500],   # temp_node_2\n",
    "[                  r'$alpha$',  1.0, 2.0],   # temp_node_4\n",
    "[                  r'$log_delta$', 3.0, 8.0],   # temp_node_3\n",
    "[                  r'$log_Pquench$', -6.0, 3.0],   # temp_node_6\n",
    "[                  r'$logFe$',  -2.3, 1.0], # CH4_mol_scale\n",
    "[                  r'$fsed$',  0.0, 10.0],   # temp_node_8\n",
    "[                  r'$logKzz$',  5.0, 13.0], # H2O_mol_scale \\_mol\\_scale\n",
    "[                  r'$sigmalnorm$',  1.05, 3.0], # C2O_mol_scale\n",
    "[                  r'$log\\_iso\\_rat$',  -11.0, -1.0],   # temp_node_7\n",
    "[                  r'$R\\_P$', 0.8, 2.0],             # R_P / R_Jupyter\n",
    "[                  r'$rv$',  10.0, 30.0], # NH3_mol_scale 20, 35\n",
    "[                  r'$vsini$', 0.0, 50 ], # H2S_mol_scale 10.0, 30.0\n",
    "[                  r'$limb\\_dark$',  0.0, 1.0], # PH3_mol_scale\n",
    "[                  r'$b$',  1, 20.0], # PH3_mol_scale\n",
    "\n",
    "])\n",
    "\n",
    "scratch = os.environ['SCRATCH']\n",
    "datapath = Path(scratch) / 'highres-sbi/data_nic5'\n",
    "savepath = Path(scratch) / 'highres-sbi/runs/sweep_lognormnoise'\n",
    "\n",
    "processing = ProcessSpec()\n",
    "d = Data()\n",
    "sim = SpectrumMaker(wavelengths=d.model_wavelengths, param_set=param_set, lbl_opacity_sampling=2)\n",
    "\n",
    "\n",
    "def simulator(theta):\n",
    "    values = theta[:-4].numpy()\n",
    "    values_ext = theta[-4:].numpy()\n",
    "    # print(values, values_ext)\n",
    "    values_actual = deNormVal(values, param_list)\n",
    "    spectrum = sim(values_actual)\n",
    "    spec = np.vstack((np.array(spectrum), d.model_wavelengths))\n",
    "    \n",
    "    values_ext_actual = deNormVal(values_ext, param_list_ext)\n",
    "    # params_ext = param_set_ext.param_dict(values_ext_actual)\n",
    "    \n",
    "    th, x = processing(torch.Tensor([values_actual]), torch.Tensor(spec), sample= False, \\\n",
    "                       values_ext_actual= torch.Tensor([values_ext_actual]))\n",
    "    # print(np.shape(x))\n",
    "    \n",
    "    return x.squeeze()\n",
    "\n",
    "\n",
    "# CONFIGS = {\n",
    "#     'embedding': ['shallow', 'deep'],\n",
    "#     'flow': ['MAF'],  #, 'NCSF', 'SOSPF', 'UNAF', 'CNF'], #'NAF', \n",
    "#     'transforms': [3, 5, 7], #, 7], #3, \n",
    "#     # 'signal': [16, 32],  # not important- the autoregression network output , 32\n",
    "#     'hidden_features': [256, 512], # hidden layers of the autoregression network , 256, \n",
    "#     'hidden_features_no' : [3,5], \n",
    "#     'activation': [nn.ELU], #, nn.ReLU],\n",
    "#     'optimizer': ['AdamW'],\n",
    "#     'init_lr':  [1e-3, 1e-4, 1e-5], #[5e-4, 1e-5]\n",
    "#     'weight_decay': [0, 1e-4, 1e-3, 1e-2], #[1e-4], #\n",
    "#     'scheduler': ['ReduceLROnPlateau'], #, 'CosineAnnealingLR'],\n",
    "#     'min_lr': [1e-5, 1e-6], # 1e-6\n",
    "#     'patience': [16, 32], #8\n",
    "#     'epochs': [2001],\n",
    "#     'stop_criterion': ['early'], #, 'late'],\n",
    "#     'batch_size':  [256, 512, 1024, 2048],\n",
    "#     'spectral_length' : [6144], #[1536, 3072, 6144]\n",
    "#     'factor' : [0.7, 0.5, 0.3], \n",
    "#     'noise' : ['lognormaldist'], #, 'Mikelineb'], uniformdist\n",
    "#     # 'SOSF_degree' : [2,3,4],\n",
    "#     # 'SOSF_poly' : [2,4,6],\n",
    "#     'gradient_steps_train': [1024],\n",
    "#     'gradient_steps_valid' : [256], \n",
    "# }\n",
    "\n",
    "## Loading from a model to plot\n",
    "CONFIGS = {\n",
    "    'embedding': ['shallow'],\n",
    "    'flow': ['MAF'],  #, 'NCSF', 'SOSPF', 'UNAF', 'CNF'], #'NAF', \n",
    "    'transforms': [3], #, 7], #3, \n",
    "    # 'signal': [16, 32],  # not important- the autoregression network output , 32\n",
    "    'hidden_features': [512], # hidden layers of the autoregression network , 256, \n",
    "    'hidden_features_no' : [5], \n",
    "    'activation': [nn.ELU], #, nn.ReLU],\n",
    "    'optimizer': ['AdamW'],\n",
    "    'init_lr':  [1e-3], #[5e-4, 1e-5]\n",
    "    'weight_decay': [1e-4], #[1e-4], #\n",
    "    'scheduler': ['ReduceLROnPlateau'], #, 'CosineAnnealingLR'],\n",
    "    'min_lr': [1e-5], # 1e-6\n",
    "    'patience': [16], #8\n",
    "    'epochs': [100],\n",
    "    'stop_criterion': ['early'], #, 'late'],\n",
    "    'batch_size':  [256],\n",
    "    'spectral_length' : [6144], #[1536, 3072, 6144]\n",
    "    'factor' : [0.3], \n",
    "    'noise_scaling' : [2], \n",
    "    'noise' : ['lognormaldist']\n",
    "    # 'SOSF_degree' : [2,3,4],\n",
    "    # 'SOSF_poly' : [2,4,6],\n",
    "}\n",
    "\n",
    "\n",
    "# @job(array=1, cpus=2, gpus=1, ram='64GB', time='10-00:00:00')\n",
    "def experiment(index: int) -> None:\n",
    "    # Config\n",
    "    config = {\n",
    "        key: random.choice(values)\n",
    "        for key, values in CONFIGS.items()\n",
    "    }\n",
    "    \n",
    "    # run = wandb.init(project='highres--sweep-lognormnoise', config=config)\n",
    "\n",
    "    # def noisybfactor(x: Tensor) -> Tensor:\n",
    "    #     #sample b from a uniform distribution, but what priors should they have? \n",
    "    #     b_factor = 10**b \n",
    "    #     error = torch.Tensor(Data().err) * torch.randn_like(x)\n",
    "    #     tens = torch.add(torch.permute(error**2, (1,0)), b_factor)\n",
    "    #     tens = torch.permute(tens, (1,0))\n",
    "    #     error_new = torch.sqrt(tens) * simulator.scale    \n",
    "    #     return x + error_new\n",
    "    # b = torch.unsqueeze(b,1)\n",
    "    # theta = torch.hstack((theta, b))\n",
    "\n",
    "    \n",
    "    def noisy(x, b= None): #50 is 10% of the median of the means of spectra in the training set.\n",
    "        bs = x.size()[0]\n",
    "        data_uncertainty = Data().err * Data().flux_scaling\n",
    "\n",
    "        if b == None: \n",
    "            if config['noise'] == 'uniformdist' :\n",
    "                b = 1  + torch.rand(bs) * (10-1)\n",
    "                b = torch.unsqueeze(b,1)\n",
    "            elif config['noise'] == 'lognormaldist' :\n",
    "                m = torch.distributions.log_normal.LogNormal(torch.tensor([1.5]), torch.tensor([0.5]))\n",
    "                b = m.sample([bs])\n",
    "            # elif config['noise'] == 'Mikelineb' :\n",
    "            #     data_uncertainty = noisybfactor(x) \n",
    "\n",
    "        else: \n",
    "            b = torch.Tensor([b])\n",
    "        \n",
    "#         print(b.size(), torch.from_numpy(data_uncertainty).size(), torch.randn_like(x).size())\n",
    "        x = x + torch.from_numpy(data_uncertainty).cuda() * b.cuda() * torch.randn_like(x) \n",
    "        \n",
    "        return x, b\n",
    "\n",
    "        # if b == None: \n",
    "        #     return x, b\n",
    "        # else: \n",
    "        #     return x\n",
    "\n",
    "    # l, u = torch.tensor(LOWER), torch.tensor(UPPER)\n",
    "\n",
    "    class NPEWithEmbedding(nn.Module):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "\n",
    "            # Estimator\n",
    "            if config['embedding'] == 'shallow':\n",
    "                self.embedding = ResMLP(6144, 64, hidden_features=[512] * 2 + [256] * 3 + [128] * 5, activation= nn.ELU)\n",
    "            else:\n",
    "                self.embedding = ResMLP(6144, 128, hidden_features=[512] * 3 + [256] * 5 + [128] * 7, activation= nn.ELU)\n",
    "            \n",
    "            if config['flow'] == 'NCSF':\n",
    "                self.npe = NPE(\n",
    "                    19, self.embedding.out_features,\n",
    "                    # moments=((l + u) / 2, (l - u) / 2),\n",
    "                    transforms=config['transforms'],\n",
    "                    build=NCSF,\n",
    "                    bins=config['signal'],\n",
    "                    hidden_features=[config['hidden_features']] * config['hidden_features_no'],\n",
    "                    activation=config['activation'],\n",
    "                )\n",
    "            elif config['flow'] == 'MAF':\n",
    "                self.npe = NPE(\n",
    "                    20, self.embedding.out_features,\n",
    "                    # moments=((l + u) / 2, (l - u) / 2),\n",
    "                    transforms=config['transforms'],\n",
    "                    build=MAF,\n",
    "                    # bins=config['signal'],\n",
    "                    hidden_features=[config['hidden_features']] * config['hidden_features_no'],\n",
    "                    activation=config['activation'],\n",
    "                )\n",
    "\n",
    "\n",
    "            elif config['flow'] == 'SOSPF':\n",
    "                    self.npe = NPE(\n",
    "                    19, self.embedding.out_features,\n",
    "                    # moments=((l + u) / 2, (l - u) / 2),\n",
    "                    transforms=config['transforms'],\n",
    "                    build=SOSPF,\n",
    "                    degree = config['SOSF_degree'],\n",
    "                    polynomials = config['SOSF_poly'],\n",
    "                    # signal=config['signal'],\n",
    "                    # hidden_features=[config['hidden_features']] * config['hidden_features_no'],\n",
    "                    # activation=config['activation'],\n",
    "                )\n",
    "                    \n",
    "            elif config['flow'] == 'UNAF':\n",
    "                    self.npe = NPE(\n",
    "                    19, self.embedding.out_features,\n",
    "                    # moments=((l + u) / 2, (l - u) / 2),\n",
    "                    transforms=config['transforms'],\n",
    "                    build=UNAF,\n",
    "                    signal=config['signal'],\n",
    "                    hidden_features=[config['hidden_features']] * config['hidden_features_no'],\n",
    "                    activation=config['activation'],\n",
    "                )\n",
    "            \n",
    "            elif config['flow'] == 'CNF':\n",
    "                    self.npe = NPE(\n",
    "                    19, self.embedding.out_features,\n",
    "                    # moments=((l + u) / 2, (l - u) / 2),\n",
    "                    transforms=config['transforms'],\n",
    "                    build=CNF,\n",
    "                    # signal=config['signal'],\n",
    "                    # hidden_features=[config['hidden_features']] * config['hidden_features_no'],\n",
    "                    # activation=config['activation'],\n",
    "                )\n",
    "            \n",
    "\n",
    "        # def forward(self, theta: Tensor, x: Tensor) -> Tensor:\n",
    "        def forward(self, theta, x): # -> Tensor:\n",
    "            y = self.embedding(x)\n",
    "            return self.npe(theta, y)\n",
    "\n",
    "        # def flow(self, x: Tensor):  # -> Distribution\n",
    "        def flow(self, x):  # -> Distribution\n",
    "            out = self.npe.flow(self.embedding(x)) #.to(torch.double)) #\n",
    "            return out\n",
    "\n",
    "    if (config['flow'] == 'SOSPF') | (config['flow'] == 'UNAF'):\n",
    "        estimator = NPEWithEmbedding().cuda()\n",
    "    \n",
    "    estimator = NPEWithEmbedding().double().cuda()\n",
    "\n",
    "    # # Optimizer\n",
    "    # loss = NPELoss(estimator)\n",
    "    # optimizer = optim.AdamW(estimator.parameters(), lr=config['init_lr'], weight_decay=config['weight_decay'])\n",
    "    # scheduler = sched.ReduceLROnPlateau(optimizer, factor= config['factor'], min_lr=config['min_lr'], patience=config['patience'], threshold=1e-2, threshold_mode='abs')\n",
    "    # step = GDStep(optimizer, clip=1)\n",
    "\n",
    "    # # Data\n",
    "    # trainset = H5Dataset(datapath / 'train.h5', batch_size=config['batch_size'], shuffle=True)\n",
    "    # validset = H5Dataset(datapath / 'valid.h5', batch_size=config['batch_size'], shuffle=True)\n",
    "\n",
    "    # # Training\n",
    "    def pipe(theta: Tensor, x: Tensor) -> Tensor:\n",
    "        theta, x = theta.cuda(), x.cuda()\n",
    "        x , b = noisy(x)\n",
    "        theta = torch.hstack((theta, b.cuda()))\n",
    "        return loss(theta, x)\n",
    "    \n",
    "    def pipeout(theta: Tensor, x: Tensor) -> Tensor:\n",
    "        theta, x = theta.cuda(), x.cuda()\n",
    "        x , b = noisy(x)\n",
    "        theta = torch.hstack((theta, b.cuda()))\n",
    "        return theta, x\n",
    "\n",
    "    # for epoch in tqdm(range(config['epochs']), unit='epoch'): #config['epochs']\n",
    "    #     estimator.train()\n",
    "        \n",
    "    #     start = time.time()\n",
    "    #     losses = torch.stack([\n",
    "    #         step(pipe(theta.float(), x.float()))  #[:,0]\n",
    "    #         for theta, x in islice(trainset, 1024) #770 1024 256\n",
    "    #     ]).cpu().numpy()\n",
    "    #     end = time.time()\n",
    "        \n",
    "    #     estimator.eval()\n",
    "        \n",
    "    #     with torch.no_grad():            \n",
    "    #         losses_val = torch.stack([\n",
    "    #             pipe(theta.float(), x.float())  #[:,0]\n",
    "    #             for theta, x in islice(validset, 256) #90 256 32\n",
    "    #         ]).cpu().numpy()\n",
    "\n",
    "    #     run.log({\n",
    "    #         'lr': optimizer.param_groups[0]['lr'],\n",
    "    #         'loss': np.nanmean(losses),\n",
    "    #         'loss_val': np.nanmean(losses_val),\n",
    "    #         'nans': np.isnan(losses).mean(),\n",
    "    #         'nans_val': np.isnan(losses_val).mean(),\n",
    "    #         'speed': len(losses) / (end - start),\n",
    "    #         'trainigset_len' :  len(losses),\n",
    "    #         'validationset_len' : len(losses_val),\n",
    "    #     })\n",
    "\n",
    "    #     scheduler.step(np.nanmean(losses_val))\n",
    "\n",
    "    #     runpath = savepath / f'{run.name}' #_{run.id}'\n",
    "    #     runpath.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    #     if epoch % 50 ==0 : \n",
    "    #             torch.save({\n",
    "    #                     'estimator': estimator.state_dict(),\n",
    "    #                     'optimizer': optimizer.state_dict(),\n",
    "    #         },  runpath / f'states_{epoch}.pth')\n",
    "\n",
    "    #     if config['stop_criterion'] == 'early' and optimizer.param_groups[0]['lr'] <= config['min_lr']:\n",
    "    #         break\n",
    "\n",
    "############################################################\n",
    "    # datapath = Path(scratch) / 'highres-sbi/data_nic5'\n",
    "    # savepath = Path(scratch) / 'highres-sbi/runs/sweep_lessnoisy'\n",
    "    # Loading from a model to plot\n",
    "    m = 'peachy-feather-81' #'comfy-dawn-59'\n",
    "    epoch = config['epochs']\n",
    "    runpath = savepath / m\n",
    "    runpath.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    estimator = NPEWithEmbedding().double()\n",
    "    states = torch.load(runpath / ('states_' + str(epoch) + '.pth'), map_location='cpu')\n",
    "    estimator.load_state_dict(states['estimator'])\n",
    "    estimator.cuda().eval()\n",
    "\n",
    "############################################################\n",
    "        \n",
    "    savepath_plots = runpath  / ('plots_sim_b_' + str(epoch))\n",
    "    savepath_plots.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "####################################################################################################################\n",
    "    # Evaluation\n",
    "    plt.rcParams.update(nice_rc(latex=True))\n",
    "\n",
    "    # Coverage\n",
    "    testset = H5Dataset(datapath / 'test.h5', batch_size=2**4) #**4\n",
    "    ranks = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for theta, x in tqdm(islice(testset, 2**8)): #**8\n",
    "            theta, x = theta.cuda(), x.cuda()\n",
    "            # x = x[:,0]\n",
    "            theta, x = pipeout(theta, x)\n",
    "            # x, _ = noisy(x,2)\n",
    "            posterior = estimator.flow(x)\n",
    "            samples = posterior.sample((2**10,))\n",
    "            log_p = posterior.log_prob(theta)\n",
    "            log_p_samples = posterior.log_prob(samples)\n",
    "\n",
    "            ranks.append((log_p_samples < log_p).float().mean(dim=0).cpu())\n",
    "\n",
    "    ranks = torch.cat(ranks)\n",
    "    ranks_numpy = ranks.double().numpy() #convert to Numpy array\n",
    "    df_ranks = pd.DataFrame(ranks_numpy) #convert to a dataframe\n",
    "    df_ranks.to_csv(savepath_plots /\"ranks.csv\",index=False) #save to file\n",
    "\n",
    "    df_ranks = pd.read_csv(savepath_plots/\"ranks.csv\")\n",
    "    ranks = df_ranks.values\n",
    "\n",
    "    a=[]\n",
    "    r = np.sort(np.asarray(ranks))\n",
    "\n",
    "    for alpha in np.linspace(0,1,100):\n",
    "        a.append((r > (1-alpha)).mean())\n",
    "\n",
    "    cov_fig, ax = plt.subplots(figsize=(4, 4))\n",
    "    ax.set_xlabel(r'Credibility level $1-\\alpha$', fontsize = 10)\n",
    "    ax.set_ylabel(r'Coverage probability', fontsize= 10)\n",
    "    ax.plot(np.linspace(0,1,100),a, color='steelblue', label='upper right') #a[::-1]\n",
    "    ax.plot([0, 1], [0, 1], color='k', linestyle='--')\n",
    "    # plt.grid()\n",
    "    # ax.set_xticks(fontsize=8)\n",
    "    # ax.set_yticks(fontsize=8)\n",
    "    cov_fig.savefig(savepath_plots / 'coverage.pdf') \n",
    "\n",
    "# ####################################################################################################\n",
    "\n",
    "    def thetascalebackup(theta):\n",
    "         #almost same as deNormVal(outputs a list not tensor)\n",
    "        theta[:, :-1] =  torch.Tensor(LOWER[:-1]) + theta[:, :-1] * (torch.Tensor(UPPER[:-1]) - torch.Tensor(LOWER[:-1]))\n",
    "        return theta\n",
    "\n",
    "# #     ## Corner    \n",
    "    x_star, _ =  noisy(torch.Tensor(np.loadtxt('/home/mvasist/Highres/observation/simulated_obs/x_sim_b.npy'))[0].cuda())\n",
    "    # x_star, _ =  noisy(torch.Tensor(np.loadtxt('x_sim_b.npy'))[0].cuda(),2)\n",
    "    theta_star = torch.Tensor(np.loadtxt('/home/mvasist/Highres/observation/simulated_obs/theta_sim_b.npy'))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        theta = torch.cat([estimator.flow(x_star.cuda()).sample((2**14,)).cpu() #**14\n",
    "                            for _ in tqdm(range(2**6))\n",
    "            \n",
    "                    ])\n",
    "\n",
    "#     ##Saving to file\n",
    "    theta_numpy = theta.double().numpy() #convert to Numpy array\n",
    "    df_theta = pd.DataFrame(theta_numpy) #convert to a dataframe\n",
    "    df_theta.to_csv( savepath_plots / 'theta.csv' ,index=False) #save to file\n",
    "    \n",
    "    #Then, to reload:\n",
    "    df_theta = pd.read_csv( savepath_plots / 'theta.csv')\n",
    "    theta = df_theta.values\n",
    "    theta = torch.from_numpy(theta)\n",
    "    # print(theta)\n",
    "    \n",
    "    corner_fig = corner_mod([thetascalebackup(theta)], legend=['NPE'], \\\n",
    "                    color= ['steelblue'] , figsize=(19,19), \\\n",
    "                 domain = (LOWER, UPPER), labels= LABELS) #\n",
    "    mark_point(corner_fig, thetascalebackup(theta_star), color='black')\n",
    "    corner_fig.savefig(savepath_plots / 'corner.pdf')\n",
    "    \n",
    "\n",
    "    ####################################################################################################\n",
    "#     ## NumPy\n",
    "    def filter_limbdark_mask(theta):\n",
    "        mask = theta[:,-2]<0\n",
    "        mask += theta[:,-2]>1\n",
    "        return mask \n",
    "\n",
    "    # print(thetascalebackup(theta))\n",
    "    mask = filter_limbdark_mask(thetascalebackup(theta))\n",
    "    theta_filterLD = theta[~mask]\n",
    "    # print(theta_filterLD)\n",
    "\n",
    "\n",
    "### PT profile\n",
    "    fig, ax = plt.subplots(figsize=(4,4))\n",
    "\n",
    "    ##sim PT\n",
    "    pressures = sim.atmosphere.press / 1e6\n",
    "    val_act = deNormVal(theta_star.numpy(), param_list)\n",
    "    params = param_set.param_dict(val_act)\n",
    "    temp= make_pt(params , pressures)\n",
    "    ax.plot(temp, pressures, color = 'black')  ##sim\n",
    "    ##sim PT\n",
    "    \n",
    "    fig_pt = PT_plot(fig, ax, theta_filterLD[:2**8, :-1], invert = True) #, self.theta_star)\n",
    "    # fig_pt = PT_plot(fig_pt, ax, self.theta_paul[:2**8], invert = True, color = 'orange') #, theta_star)\n",
    "    # fig_pt.savefig(self.savepath_plots / 'pt_profile_Paul_unregPTwithb_24Apr2023.pdf')\n",
    "    fig_pt.savefig(savepath_plots / 'pt_profile.pdf')\n",
    "\n",
    "####################################################################################################\n",
    "\n",
    "    # ## Residuals\n",
    "    theta_filterLD_512 = theta_filterLD[:2**9]\n",
    "    x_filterLD_512 = np.stack([simulator(t) for t in tqdm(theta_filterLD_512[:,:-1])]) #**9\n",
    "    x_filterLD_512 = x_filterLD_512[:,0]\n",
    "    mask = ~np.isnan(x_filterLD_512).any(axis=-1)\n",
    "    mask1 = ~np.isinf(x_filterLD_512[mask]).any(axis=-1)\n",
    "    theta_filterLD_512, x_filterLD_512 = theta_filterLD_512[mask][mask1], x_filterLD_512[mask][mask1]\n",
    "    x_filterLD_512 = torch.from_numpy(x_filterLD_512)\n",
    "    x_filterLD_512, _= noisy(x_filterLD_512.cuda(), theta_filterLD_512[:, -1])\n",
    "    # x_filterLD_512, _= noisy(x_filterLD_512.cuda(), theta_filterLD_512[:, -1])\n",
    "\n",
    "    df_theta = pd.DataFrame(theta_filterLD_512) #convert to a dataframe\n",
    "    df_x = pd.DataFrame(x_filterLD_512.cpu()) #convert to a dataframe\n",
    "\n",
    "    df_theta.to_csv('theta_256_noisy.csv',index=False) #save to file\n",
    "    df_x.to_csv('x_256_noisy.csv',index=False) #save to file\n",
    "\n",
    "    #Then, to reload:\n",
    "    df_theta = pd.read_csv('theta_256_noisy.csv')\n",
    "    theta_256_noisy = df_theta.values\n",
    "    df_x = pd.read_csv('x_256_noisy.csv')\n",
    "    x = df_x.values\n",
    "    theta_256_noisy, x_256_noisy = torch.from_numpy(theta_256_noisy), torch.from_numpy(x)\n",
    "\n",
    "    res_fig, (ax1, ax2) = plt.subplots(2, figsize=(10,7), gridspec_kw={'height_ratios': [3, 1]})\n",
    "    creds= [0.997, 0.955, 0.683]\n",
    "    alpha = (0.0, 0.9)\n",
    "    levels, creds = levels_and_creds(creds= creds, alpha = alpha)\n",
    "    cmap= LinearAlphaColormap('steelblue', levels=creds, alpha=alpha)\n",
    "\n",
    "    wlength = d.data_wavelengths\n",
    "\n",
    "    for q, l in zip(creds[:-1], levels):\n",
    "        lower, upper = np.quantile(x_256_noisy.numpy(), [0.5 - q / 2, 0.5 + q / 2], axis=0)\n",
    "        ax1.fill_between(wlength, lower, upper, color= cmap(l), linewidth=0) #'C0', alpha=0.4,\n",
    "\n",
    "    lines = ax1.plot(wlength, x_star.cpu(), color='black', label = r'$ f(\\theta_{obs})$', linewidth = 0.4)\n",
    "    handles, texts = legends(axes= ax1, alpha=alpha) #0.15, 0.75\n",
    "    texts = [r'$ f(\\theta_{obs})$', r'$p_{\\phi}(f(\\theta)|x_{obs})$']\n",
    "\n",
    "    plt.setp(ax1.get_xticklabels(), visible=False)\n",
    "    ax1.set_ylabel(r'Planet flux $F_\\nu$ (10$^{-5}$) Jy', fontsize = 10)\n",
    "    ax1.legend(handles, texts, prop = {'size': 8}, bbox_to_anchor=(1,1))\n",
    "\n",
    "    residuals = (x_256_noisy - x_star.cpu()) / torch.Tensor(d.err*d.flux_scaling*config['noise_scaling'])\n",
    "\n",
    "    for q, l in zip(creds[:-1], levels):\n",
    "        lower, upper = np.quantile(residuals, [0.5 - q / 2, 0.5 + q / 2], axis=0)\n",
    "        ax2.fill_between(wlength, lower, upper, color= cmap(l) , linewidth=0) \n",
    "    ax2.set_ylabel(r'Residuals', fontsize = 10)\n",
    "    ax2.set_xlabel( r'Wavelength ($\\mu$m)', fontsize = 10)\n",
    "    res_fig.savefig(savepath_plots / 'consistency_noisy.pdf')\n",
    "\n",
    "\n",
    "    # run.log({\n",
    "    #     'coverage': wandb.Image(cov_fig),\n",
    "    #     'corner': wandb.Image(corner_fig),\n",
    "    #     'pt_profile': wandb.Image(fig_pt),\n",
    "    #     'res_fig': wandb.Image(res_fig),\n",
    "    # })\n",
    "    # run.finish()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 1]) torch.Size([6144]) torch.Size([16, 6144])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "1it [00:09,  9.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 1]) torch.Size([6144]) torch.Size([16, 6144])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "2it [00:15,  7.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 1]) torch.Size([6144]) torch.Size([16, 6144])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "3it [00:22,  7.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 1]) torch.Size([6144]) torch.Size([16, 6144])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "4it [00:28,  6.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 1]) torch.Size([6144]) torch.Size([16, 6144])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "5it [00:35,  6.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 1]) torch.Size([6144]) torch.Size([16, 6144])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "6it [00:42,  6.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 1]) torch.Size([6144]) torch.Size([16, 6144])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "7it [00:48,  6.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 1]) torch.Size([6144]) torch.Size([16, 6144])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "8it [00:55,  6.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 1]) torch.Size([6144]) torch.Size([16, 6144])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "9it [01:02,  6.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 1]) torch.Size([6144]) torch.Size([16, 6144])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "10it [01:08,  6.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 1]) torch.Size([6144]) torch.Size([16, 6144])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "11it [01:15,  6.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 1]) torch.Size([6144]) torch.Size([16, 6144])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "12it [01:22,  6.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 1]) torch.Size([6144]) torch.Size([16, 6144])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "13it [01:29,  6.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 1]) torch.Size([6144]) torch.Size([16, 6144])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "14it [01:35,  6.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 1]) torch.Size([6144]) torch.Size([16, 6144])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "15it [01:42,  6.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 1]) torch.Size([6144]) torch.Size([16, 6144])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15it [01:42,  6.86s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mexperiment\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[4], line 384\u001b[0m, in \u001b[0;36mexperiment\u001b[0;34m(index)\u001b[0m\n\u001b[1;32m    382\u001b[0m \u001b[38;5;66;03m# x, _ = noisy(x,2)\u001b[39;00m\n\u001b[1;32m    383\u001b[0m posterior \u001b[38;5;241m=\u001b[39m estimator\u001b[38;5;241m.\u001b[39mflow(x)\n\u001b[0;32m--> 384\u001b[0m samples \u001b[38;5;241m=\u001b[39m \u001b[43mposterior\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    385\u001b[0m log_p \u001b[38;5;241m=\u001b[39m posterior\u001b[38;5;241m.\u001b[39mlog_prob(theta)\n\u001b[1;32m    386\u001b[0m log_p_samples \u001b[38;5;241m=\u001b[39m posterior\u001b[38;5;241m.\u001b[39mlog_prob(samples)\n",
      "File \u001b[0;32m~/miniconda3/envs/HighResear/lib/python3.9/site-packages/torch/distributions/distribution.py:151\u001b[0m, in \u001b[0;36mDistribution.sample\u001b[0;34m(self, sample_shape)\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;124;03mGenerates a sample_shape shaped sample or sample_shape shaped batch of\u001b[39;00m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;124;03msamples if the distribution parameters are batched.\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 151\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrsample\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample_shape\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/HighResear/lib/python3.9/site-packages/zuko/distributions.py:121\u001b[0m, in \u001b[0;36mNormalizingFlow.rsample\u001b[0;34m(self, shape)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    119\u001b[0m     z \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase\u001b[38;5;241m.\u001b[39msample(shape)\n\u001b[0;32m--> 121\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mz\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/HighResear/lib/python3.9/site-packages/torch/distributions/transforms.py:251\u001b[0m, in \u001b[0;36m_InverseTransform.__call__\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m    250\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inv \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inv_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/HighResear/lib/python3.9/site-packages/torch/distributions/transforms.py:159\u001b[0m, in \u001b[0;36mTransform._inv_call\u001b[0;34m(self, y)\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    156\u001b[0m \u001b[38;5;124;03mInverts the transform `y => x`.\u001b[39;00m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cache_size \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 159\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inverse\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    160\u001b[0m x_old, y_old \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cached_x_y\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m y_old:\n",
      "File \u001b[0;32m~/miniconda3/envs/HighResear/lib/python3.9/site-packages/zuko/transforms.py:112\u001b[0m, in \u001b[0;36mComposedTransform._inverse\u001b[0;34m(self, y)\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_inverse\u001b[39m(\u001b[38;5;28mself\u001b[39m, y: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m    111\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mreversed\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms):\n\u001b[0;32m--> 112\u001b[0m         y \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minv\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    113\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m y\n",
      "File \u001b[0;32m~/miniconda3/envs/HighResear/lib/python3.9/site-packages/torch/distributions/transforms.py:251\u001b[0m, in \u001b[0;36m_InverseTransform.__call__\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m    250\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inv \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inv_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/HighResear/lib/python3.9/site-packages/torch/distributions/transforms.py:159\u001b[0m, in \u001b[0;36mTransform._inv_call\u001b[0;34m(self, y)\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    156\u001b[0m \u001b[38;5;124;03mInverts the transform `y => x`.\u001b[39;00m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cache_size \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 159\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inverse\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    160\u001b[0m x_old, y_old \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cached_x_y\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m y_old:\n",
      "File \u001b[0;32m~/miniconda3/envs/HighResear/lib/python3.9/site-packages/zuko/transforms.py:697\u001b[0m, in \u001b[0;36mAutoregressiveTransform._inverse\u001b[0;34m(self, y)\u001b[0m\n\u001b[1;32m    695\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros_like(y)\n\u001b[1;32m    696\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpasses):\n\u001b[0;32m--> 697\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmeta\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39minv(y)\n\u001b[1;32m    699\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/miniconda3/envs/HighResear/lib/python3.9/site-packages/zuko/flows.py:310\u001b[0m, in \u001b[0;36mMaskedAutoregressiveTransform.meta\u001b[0;34m(self, y, x)\u001b[0m\n\u001b[1;32m    306\u001b[0m     x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(broadcast(x, y, ignore\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    308\u001b[0m total \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msizes)\n\u001b[0;32m--> 310\u001b[0m phi \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhyper\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    311\u001b[0m phi \u001b[38;5;241m=\u001b[39m phi\u001b[38;5;241m.\u001b[39munflatten(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, (phi\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m total, total))\n\u001b[1;32m    312\u001b[0m phi \u001b[38;5;241m=\u001b[39m phi\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msizes, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/HighResear/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/HighResear/lib/python3.9/site-packages/torch/nn/modules/container.py:139\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 139\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/HighResear/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/HighResear/lib/python3.9/site-packages/zuko/nn.py:127\u001b[0m, in \u001b[0;36mMaskedLinear.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 127\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mlinear(x, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "experiment(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:HighResear]",
   "language": "python",
   "name": "conda-env-HighResear-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
